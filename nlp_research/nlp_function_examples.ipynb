{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb7f09b-7d4d-4a6e-9732-9fb561f71383",
   "metadata": {},
   "source": [
    "# Examples of NLP Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c9f461-9de3-4309-8e6f-e627783a0895",
   "metadata": {},
   "source": [
    "This jupyter noteook contains example code to generate linguistic features with the functions in nlp_research.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bac88e3-53c2-44ae-951e-4293fe369406",
   "metadata": {},
   "source": [
    "## `data_to_df()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a5bc55-ac01-429d-a9ba-f03b2af8eb74",
   "metadata": {},
   "source": [
    "This function creates an easy way to access and visualize parts-of-speech tagging with spacy.\n",
    "The tags and values are stored in a pandas dataframe.\n",
    "Given a spacy pipeline and filepath, the function will output a dataframe showing parts-of-speech tags and their values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42f1a91-2c19-4747-96a2-fcb5566b5ea5",
   "metadata": {},
   "source": [
    "Run the following codeblock to install spacy and pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7924a3f-07ae-47a5-98de-2e092b00f522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hbmcmaho\\natural-language-processing\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdfbcb29-957d-418a-abd3-c65a6cba102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_functions import data_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a75c08-59e9-4d57-bb5f-6ca91293e0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LEMMA</th>\n",
       "      <th>POS</th>\n",
       "      <th>TAG</th>\n",
       "      <th>DEP</th>\n",
       "      <th>SHAPE</th>\n",
       "      <th>ALPHA</th>\n",
       "      <th>STOP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You</td>\n",
       "      <td>you</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>Xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>know</td>\n",
       "      <td>know</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBP</td>\n",
       "      <td>parataxis</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "      <td>,</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>WRB</td>\n",
       "      <td>advmod</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>X</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>everyone</td>\n",
       "      <td>everyone</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>knew</td>\n",
       "      <td>know</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBD</td>\n",
       "      <td>relcl</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>your</td>\n",
       "      <td>your</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>poss</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>name</td>\n",
       "      <td>name</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>dobj</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TEXT     LEMMA    POS   TAG        DEP SHAPE  ALPHA   STOP\n",
       "0        You       you   PRON   PRP      nsubj   Xxx   True   True\n",
       "1       know      know   VERB   VBP  parataxis  xxxx   True  False\n",
       "2          ,         ,  PUNCT     ,      punct     ,  False  False\n",
       "3       when      when  SCONJ   WRB     advmod  xxxx   True   True\n",
       "4          I         I   PRON   PRP      nsubj     X   True   True\n",
       "..       ...       ...    ...   ...        ...   ...    ...    ...\n",
       "61  everyone  everyone   PRON    NN      nsubj  xxxx   True   True\n",
       "62      knew      know   VERB   VBD      relcl  xxxx   True  False\n",
       "63      your      your   PRON  PRP$       poss  xxxx   True   True\n",
       "64      name      name   NOUN    NN       dobj  xxxx   True   True\n",
       "65         .         .  PUNCT     .      punct     .  False  False\n",
       "\n",
       "[66 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_df(pipeline=\"en_core_web_lg\", file_path=\"sample.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb6b40-945c-4f82-abfd-63b209a398e4",
   "metadata": {},
   "source": [
    "## `tag_ratio()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed214b8-a2b4-496c-a92a-3650a0c8bbd1",
   "metadata": {},
   "source": [
    "This function allows the user to tag parts of speech and other attributes of the tokens in their text.\n",
    "Each column in the above data frame can be passed to the tag argument in the function.\n",
    "The function then creates and outputs a dictionary containing all instances of the categories and on average how many are present per 100 words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b026291-b82a-4e13-8daa-72e55b567930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_functions import tag_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e21328-597c-422a-bd40-519073c774d5",
   "metadata": {},
   "source": [
    "### Tagging parts-of-speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a9471f-9e15-408a-a5cb-577116132b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'PRON': 15.151515151515152,\n",
       "             'VERB': 13.636363636363635,\n",
       "             'PUNCT': 12.121212121212121,\n",
       "             'SCONJ': 4.545454545454546,\n",
       "             'ADV': 7.575757575757576,\n",
       "             'ADP': 9.090909090909092,\n",
       "             'NOUN': 15.151515151515152,\n",
       "             'AUX': 3.0303030303030303,\n",
       "             'ADJ': 6.0606060606060606,\n",
       "             'DET': 7.575757575757576,\n",
       "             'PROPN': 1.5151515151515151,\n",
       "             'PART': 1.5151515151515151,\n",
       "             'CCONJ': 1.5151515151515151,\n",
       "             'NUM': 1.5151515151515151})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_ratio(pipeline='en_core_web_lg', file_path=\"sample.txt\", tag=\"POS\", amount=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51168a44-8030-4125-8903-ddfc4a996878",
   "metadata": {},
   "source": [
    "### Tagging parts-of-speech using the Penn Treebank tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3ba725c-9e48-45bb-804f-8eb1c9b26e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'PRP': 8.547008547008547,\n",
       "             'VBP': 1.7094017094017095,\n",
       "             ',': 6.1253561253561255,\n",
       "             'WRB': 1.566951566951567,\n",
       "             'RB': 5.413105413105413,\n",
       "             'IN': 8.547008547008547,\n",
       "             'PRP$': 1.9943019943019942,\n",
       "             'NN': 13.96011396011396,\n",
       "             'VBZ': 1.7094017094017095,\n",
       "             'JJ': 5.555555555555555,\n",
       "             'DT': 8.262108262108262,\n",
       "             'NNS': 4.700854700854701,\n",
       "             'VB': 2.564102564102564,\n",
       "             'WP': 0.5698005698005698,\n",
       "             '.': 4.843304843304843,\n",
       "             'VBD': 7.4074074074074066,\n",
       "             'RP': 0.5698005698005698,\n",
       "             'VBN': 0.7122507122507122,\n",
       "             'NNP': 1.4245014245014245,\n",
       "             ':': 1.282051282051282,\n",
       "             'VBG': 2.849002849002849,\n",
       "             'CC': 4.273504273504273,\n",
       "             'CD': 0.14245014245014245,\n",
       "             'HYPH': 0.2849002849002849,\n",
       "             '_SP': 0.8547008547008548,\n",
       "             'NFP': 0.14245014245014245,\n",
       "             'WDT': 0.5698005698005698,\n",
       "             'RBR': 0.2849002849002849,\n",
       "             'TO': 0.9971509971509971,\n",
       "             'JJS': 0.2849002849002849,\n",
       "             'POS': 0.14245014245014245,\n",
       "             'MD': 0.9971509971509971,\n",
       "             'JJR': 0.5698005698005698,\n",
       "             'EX': 0.14245014245014245})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_ratio(pipeline='en_core_web_lg', file_path=\"test.txt\", tag=\"TAG\", amount=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf9538-709c-407e-b57f-f39a72ad61c8",
   "metadata": {},
   "source": [
    "### Tagging dependancies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e77d74-818a-449b-9c77-74da27569fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'nsubj': 10.683760683760683,\n",
       "             'parataxis': 0.14245014245014245,\n",
       "             'punct': 12.678062678062679,\n",
       "             'advmod': 6.267806267806268,\n",
       "             'advcl': 1.282051282051282,\n",
       "             'prep': 7.977207977207977,\n",
       "             'poss': 2.1367521367521367,\n",
       "             'pobj': 7.4074074074074066,\n",
       "             'ROOT': 4.843304843304843,\n",
       "             'acomp': 1.1396011396011396,\n",
       "             'det': 7.4074074074074066,\n",
       "             'amod': 4.5584045584045585,\n",
       "             'ccomp': 2.2792022792022792,\n",
       "             'attr': 2.2792022792022792,\n",
       "             'prt': 0.7122507122507122,\n",
       "             'acl': 0.5698005698005698,\n",
       "             'oprd': 0.14245014245014245,\n",
       "             'conj': 5.413105413105413,\n",
       "             'neg': 1.1396011396011396,\n",
       "             'appos': 0.7122507122507122,\n",
       "             'cc': 4.415954415954416,\n",
       "             'nummod': 0.14245014245014245,\n",
       "             'relcl': 1.9943019943019942,\n",
       "             'dobj': 5.270655270655271,\n",
       "             'compound': 1.9943019943019942,\n",
       "             'dep': 0.9971509971509971,\n",
       "             'csubj': 0.14245014245014245,\n",
       "             'aux': 2.2792022792022792,\n",
       "             'xcomp': 1.1396011396011396,\n",
       "             'mark': 0.4273504273504274,\n",
       "             'pcomp': 0.5698005698005698,\n",
       "             'case': 0.14245014245014245,\n",
       "             'auxpass': 0.2849002849002849,\n",
       "             'nsubjpass': 0.14245014245014245,\n",
       "             'expl': 0.14245014245014245,\n",
       "             'npadvmod': 0.14245014245014245})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_ratio(pipeline='en_core_web_lg', file_path=\"test.txt\", tag=\"DEP\", amount=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e4129-8503-431e-81b8-b65da598b114",
   "metadata": {},
   "source": [
    "## `num_tense_inflected_verbs()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce330ba7-9158-4f56-8cd7-9a3b64fe26f3",
   "metadata": {},
   "source": [
    "This function takes in a spacy pipeline, file_path, and per word amount. It loads the desired pipeline as a natural language processor and uses this to create a spacy doc object version of the file provided. \n",
    "The function then loops through the tokens in the text, filtered using `token.is_alpha` to ignore punctuation and digits. It calculates the ratio of tense inflected verbs to total words in the text, and outputs on average how many tense inflected verbs are present per specified word amount (by default: 100). \n",
    "Tense inflected verbs are considered to be verbs in past or present tense, and modal auxiliaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80507272-78ec-4399-b4f9-7b3ed8e46d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_functions import num_tense_inflected_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38e62c38-28a1-48a8-a9b3-09940500d921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following represents the average number of tenseinflected verbs present per 100 words of the given text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.285714285714285"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The following represents the average number of tenseinflected verbs present per 100 words of the given text:\")\n",
    "num_tense_inflected_verbs(pipeline='en_core_web_lg', file_path=\"sample.txt\", amount=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59ca49c-fda8-4ccd-a9ad-4db245c58224",
   "metadata": {},
   "source": [
    "## `calculate_idea_density()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c53fe5-310c-4338-b71e-4822f4d98743",
   "metadata": {},
   "source": [
    "This function takes in a pipeline and filepath and transforms into a spacy doc object, as before. The function then calculates and outputs the average idea density per sentence in the document. Idea density is defined as te number of propositions(verbs, adjectives, adverbs, prepositions, and conjunctions) divided by the number of words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3df592c0-30ea-4cb5-bbee-2036463cbf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_functions import calculate_idea_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e6db0a2-e4e4-49d0-92c8-da3f14fc2f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following represents the average idea density per sentence in the given text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.05995670995671"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The following represents the average idea density per sentence in the given text:\")\n",
    "calculate_idea_density(pipeline='en_core_web_lg', file_path=\"sample.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e87f737-d693-4f72-83ee-30becc17b10a",
   "metadata": {},
   "source": [
    "## `abstractness()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b34a040-472d-4120-87e5-d2225b70319e",
   "metadata": {},
   "source": [
    "This function takes in a pipeline and a file path and a dataset path as inputs. It loads the prefered pipeline and turns the contents of the file into a spacy doc object. \n",
    "The function then calculates the average abstractness value of all words in the text, using the inverse of the concreteness value taken from the data set. This value is returned. \n",
    "The dataset values are on a five point scale, going from abstract to concrete. For the purpose of this feature, the scale is inverted. More details on the dataset please see [this article](https://link.springer.com/article/10.3758/s13428-013-0403-5#Sec10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cab56fa-48b9-469d-9066-8d8e41376e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_functions import abstractness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6860867-dfa1-4b13-8bb4-da14336aa4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following represents the average abstractness value of all words in the text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04472862928497764"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The following represents the average abstractness value of all words in the text:\")\n",
    "abstractness(pipeline='en_core_web_lg', file_path=\"sample.txt\", dataset_path=\"datasets/dataset_for_abstractness.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8b56b2-1ef7-44e9-8ee0-687c8fe5c567",
   "metadata": {},
   "source": [
    "## `semantic_ambiguity()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b061bf-e980-477d-855c-f2b340d99397",
   "metadata": {},
   "source": [
    "This function takes in a pipeline, file path and a dataset path as inputs. It loads the prefered pipeline and turns the contents of the file into a spacy doc object. The function then calculates the average semantic ambiguity value for all words in the text using a semantic diversity value from a dataset.\n",
    "The dataset value is based on a measure that considers words that appear in a wide range of contexts on diverse topics more sematically diverse than those that appear in a restricted set of similar contexts. More details on the methods of calculation are present in [this article](https://link.springer.com/article/10.3758/s13428-012-0278-x#SecESM1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c7a1011-c551-4b30-bb34-740090e1d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_functions import semantic_ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc7df882-75d7-4dd3-8445-baea58989712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following represents the average semantic ambiguity value for all words in the provided text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3134804578708375"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The following represents the average semantic ambiguity value for all words in the provided text:\")\n",
    "semantic_ambiguity(pipeline='en_core_web_lg', file_path=\"sample.txt\", dataset_path=\"datasets/dataset_for_semantic_ambiguity.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e54f9-be11-4429-87dd-4d2c5a5ad68a",
   "metadata": {},
   "source": [
    "## `word_frequency()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b53573-e9dd-48d6-a16e-2cb90181cae9",
   "metadata": {},
   "source": [
    "This function takes in a pipeline, a file path and a dataset path as inputs. It loads the prefered pipeline and turns the contents of the file into a spacy doc object. It calculates the word frequency value of each word using a dataset, and outputs the average value of all words. More details on the dataset values are present in [this article](https://link.springer.com/article/10.3758/BRM.41.4.977#SecESM1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07829a30-8527-44b5-811a-c30b5905d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_functions import word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c4ef2b4-04e6-4e5a-8278-d11c8d5542b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the following represents the average word frequency value across all words in the given text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.641867548337011"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"the following represents the average word frequency value across all words in the given text:\")\n",
    "word_frequency(pipeline='en_core_web_lg', file_path=\"sample.txt\", dataset_path=\"datasets/dataset_for_word_frequency.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43deb445-44b8-497a-b4ea-124dc35c8a98",
   "metadata": {},
   "source": [
    "## `word_prevalence()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d4824a-f32c-4056-8f1e-07b31c092f2d",
   "metadata": {},
   "source": [
    "This function takes in a pipeline, a file path and a datset path as inputs. It loads the prefered pipeline and turns the contents of the file into a spacy doc object. The function calculates the average word prevalence value across all words in the text and returns the result. The word prevalence values are extracted from a datset. More information on them can be found in [this article](https://link.springer.com/article/10.3758/s13428-018-1077-9#Sec9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d85085c-b606-4a09-b254-3457a5c64309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_functions import word_prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba1b4ca5-65d4-4812-b825-28ef3b660fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following represents the average word prevalence value across all words in the text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4249447629308993"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The following represents the average word prevalence value across all words in the text:\")\n",
    "word_prevalence(pipeline='en_core_web_lg', file_path=\"sample.txt\", dataset_path=\"datasets/dataset_for_word_prevalence_and_familiarity.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b22db-f092-49b7-9577-138296e0a83b",
   "metadata": {},
   "source": [
    "## `word_familiarity()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb6a167-d010-4979-b3f7-3dffa544dd7b",
   "metadata": {},
   "source": [
    "This function takes in a pipeline, a file path and a dataset path as inputs. It loads the prefered pipeline and turns the contents of the file into a spacy doc object. The function calculates the average word familiarity across all words in the text and returns the result. The word familiarity values are taken from a dataset, in which they are calculated based on a z standardized measure of how many people know the word. More information on these values can be found in [this article](https://link.springer.com/article/10.3758/s13428-018-1077-9#Sec9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45f26d96-faed-457e-b377-07a0c9e56c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_functions import word_familiarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6af8f33e-6625-40ab-b82a-345aff5425cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following represents the average word familiarity value across all words in the given text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.177838994634659"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The following represents the average word familiarity value across all words in the given text:\")\n",
    "word_familiarity(pipeline='en_core_web_lg', file_path=\"sample.txt\", dataset_path=\"datasets/dataset_for_word_prevalence_and_familiarity.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a804fb-5edd-4028-8a68-28c209cf6562",
   "metadata": {},
   "source": [
    "## `age_of_acquisition()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb335f51-a9d1-4fed-9265-6e83208755a1",
   "metadata": {},
   "source": [
    "This function takes in a pipeline, a file path and a dataset path as inputs. It loads the prefered pipeline, and turns the contents of the file into a spacy doc object. The function calculates and returns the average age of acquisition value across all words in the text. This is a measure of semantic complexity, with a higher age of acquisition representing more complex language. The value is taken from a dataset, more information on this value can be found in [this article](https://link.springer.com/article/10.3758/s13428-018-1077-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25f99c53-e32e-4555-bb72-42edafac4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_functions import age_of_acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb293dbb-7d66-477c-8b8c-e9e636c2436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following represents the average age of aquisition value across all words in the text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9691179896458826"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The following represents the average age of aquisition value across all words in the text:\")\n",
    "age_of_acquisition(pipeline='en_core_web_sm', file_path=\"sample.txt\", dataset_path=\"datasets/dataset_for_age_of_acquisition.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13084a54-b9aa-40e9-9395-be636da83abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP venv)",
   "language": "python",
   "name": "nlp-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
